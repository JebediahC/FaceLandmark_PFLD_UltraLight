{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f766e3a7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-17T03:55:21.158874Z",
     "iopub.status.busy": "2022-05-17T03:55:21.158388Z",
     "iopub.status.idle": "2022-05-17T03:55:21.168564Z",
     "shell.execute_reply": "2022-05-17T03:55:21.167998Z"
    },
    "papermill": {
     "duration": 0.026586,
     "end_time": "2022-05-17T03:55:21.170305",
     "exception": false,
     "start_time": "2022-05-17T03:55:21.143719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# ls\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a93e453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T03:55:21.196062Z",
     "iopub.status.busy": "2022-05-17T03:55:21.195640Z",
     "iopub.status.idle": "2022-05-17T03:55:24.098921Z",
     "shell.execute_reply": "2022-05-17T03:55:24.098059Z"
    },
    "papermill": {
     "duration": 2.918428,
     "end_time": "2022-05-17T03:55:24.101153",
     "exception": false,
     "start_time": "2022-05-17T03:55:21.182725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'FaceLandmark_PFLD_UltraLight'...\r\n",
      "remote: Enumerating objects: 198, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (24/24), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\r\n",
      "remote: Total 198 (delta 13), reused 0 (delta 0), pack-reused 174\u001b[K\r\n",
      "Receiving objects: 100% (198/198), 19.18 MiB | 17.26 MiB/s, done.\r\n",
      "Resolving deltas: 100% (90/90), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AnthonyF333/FaceLandmark_PFLD_UltraLight.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d93d846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T03:55:24.133808Z",
     "iopub.status.busy": "2022-05-17T03:55:24.133577Z",
     "iopub.status.idle": "2022-05-17T03:55:24.139749Z",
     "shell.execute_reply": "2022-05-17T03:55:24.138813Z"
    },
    "papermill": {
     "duration": 0.02441,
     "end_time": "2022-05-17T03:55:24.141650",
     "exception": false,
     "start_time": "2022-05-17T03:55:24.117240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/FaceLandmark_PFLD_UltraLight\n"
     ]
    }
   ],
   "source": [
    "cd /kaggle/working/FaceLandmark_PFLD_UltraLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9807877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T03:55:24.173760Z",
     "iopub.status.busy": "2022-05-17T03:55:24.173209Z",
     "iopub.status.idle": "2022-05-17T03:55:34.526841Z",
     "shell.execute_reply": "2022-05-17T03:55:34.526012Z"
    },
    "papermill": {
     "duration": 10.37266,
     "end_time": "2022-05-17T03:55:34.529716",
     "exception": false,
     "start_time": "2022-05-17T03:55:24.157056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from -r requirement.txt (line 1)) (2.6.0)\r\n",
      "Requirement already satisfied: easydict in /opt/conda/lib/python3.7/site-packages (from -r requirement.txt (line 2)) (1.9)\r\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from -r requirement.txt (line 3)) (4.5.4.60)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from -r requirement.txt (line 4)) (3.5.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from -r requirement.txt (line 5)) (1.7.3)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (3.19.4)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (1.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (1.21.6)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (2.0.3)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (0.4.6)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (1.8.1)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (1.43.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (2.27.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (0.6.1)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (0.37.1)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (59.8.0)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (1.35.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r requirement.txt (line 1)) (3.3.6)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r requirement.txt (line 4)) (3.0.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r requirement.txt (line 4)) (21.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r requirement.txt (line 4)) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r requirement.txt (line 4)) (1.4.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r requirement.txt (line 4)) (4.30.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r requirement.txt (line 4)) (9.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r requirement.txt (line 4)) (2.8.2)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard->-r requirement.txt (line 1)) (1.16.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirement.txt (line 1)) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirement.txt (line 1)) (4.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirement.txt (line 1)) (4.2.4)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirement.txt (line 1)) (1.3.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->-r requirement.txt (line 4)) (4.2.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard->-r requirement.txt (line 1)) (4.11.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirement.txt (line 1)) (2.0.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirement.txt (line 1)) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirement.txt (line 1)) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirement.txt (line 1)) (1.26.8)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirement.txt (line 1)) (3.7.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r requirement.txt (line 1)) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirement.txt (line 1)) (3.2.0)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bdca293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T03:55:34.585718Z",
     "iopub.status.busy": "2022-05-17T03:55:34.585451Z",
     "iopub.status.idle": "2022-05-17T03:55:57.592934Z",
     "shell.execute_reply": "2022-05-17T03:55:57.591984Z"
    },
    "papermill": {
     "duration": 23.03869,
     "end_time": "2022-05-17T03:55:57.596133",
     "exception": false,
     "start_time": "2022-05-17T03:55:34.557443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/wider-facial-landmarks-inthewild-wflw/WFLW ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb032e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T03:55:57.653188Z",
     "iopub.status.busy": "2022-05-17T03:55:57.652919Z",
     "iopub.status.idle": "2022-05-17T03:55:57.658863Z",
     "shell.execute_reply": "2022-05-17T03:55:57.657731Z"
    },
    "papermill": {
     "duration": 0.036831,
     "end_time": "2022-05-17T03:55:57.660572",
     "exception": false,
     "start_time": "2022-05-17T03:55:57.623741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/FaceLandmark_PFLD_UltraLight/data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f53b2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T03:55:57.695666Z",
     "iopub.status.busy": "2022-05-17T03:55:57.695149Z",
     "iopub.status.idle": "2022-05-17T04:31:18.394170Z",
     "shell.execute_reply": "2022-05-17T04:31:18.393359Z"
    },
    "papermill": {
     "duration": 2120.718812,
     "end_time": "2022-05-17T04:31:18.396378",
     "exception": false,
     "start_time": "2022-05-17T03:55:57.677566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/FaceLandmark_PFLD_UltraLight/data/test_data\r\n",
      "file: 100/2500\r\n",
      "file: 200/2500\r\n",
      "file: 300/2500\r\n",
      "file: 400/2500\r\n",
      "file: 500/2500\r\n",
      "file: 600/2500\r\n",
      "file: 700/2500\r\n",
      "file: 800/2500\r\n",
      "file: 900/2500\r\n",
      "file: 1000/2500\r\n",
      "file: 1100/2500\r\n",
      "file: 1200/2500\r\n",
      "file: 1300/2500\r\n",
      "file: 1400/2500\r\n",
      "file: 1500/2500\r\n",
      "file: 1600/2500\r\n",
      "file: 1700/2500\r\n",
      "file: 1800/2500\r\n",
      "file: 1900/2500\r\n",
      "file: 2000/2500\r\n",
      "file: 2100/2500\r\n",
      "file: 2200/2500\r\n",
      "file: 2300/2500\r\n",
      "file: 2400/2500\r\n",
      "file: 2500/2500\r\n",
      "/kaggle/working/FaceLandmark_PFLD_UltraLight/data/train_data\r\n",
      "file: 100/7500\r\n",
      "file: 200/7500\r\n",
      "file: 300/7500\r\n",
      "file: 400/7500\r\n",
      "file: 500/7500\r\n",
      "file: 600/7500\r\n",
      "file: 700/7500\r\n",
      "file: 800/7500\r\n",
      "file: 900/7500\r\n",
      "file: 1000/7500\r\n",
      "file: 1100/7500\r\n",
      "file: 1200/7500\r\n",
      "file: 1300/7500\r\n",
      "file: 1400/7500\r\n",
      "file: 1500/7500\r\n",
      "file: 1600/7500\r\n",
      "file: 1700/7500\r\n",
      "file: 1800/7500\r\n",
      "file: 1900/7500\r\n",
      "file: 2000/7500\r\n",
      "file: 2100/7500\r\n",
      "file: 2200/7500\r\n",
      "file: 2300/7500\r\n",
      "file: 2400/7500\r\n",
      "file: 2500/7500\r\n",
      "file: 2600/7500\r\n",
      "file: 2700/7500\r\n",
      "file: 2800/7500\r\n",
      "file: 2900/7500\r\n",
      "file: 3000/7500\r\n",
      "file: 3100/7500\r\n",
      "file: 3200/7500\r\n",
      "file: 3300/7500\r\n",
      "file: 3400/7500\r\n",
      "file: 3500/7500\r\n",
      "file: 3600/7500\r\n",
      "file: 3700/7500\r\n",
      "file: 3800/7500\r\n",
      "file: 3900/7500\r\n",
      "file: 4000/7500\r\n",
      "file: 4100/7500\r\n",
      "file: 4200/7500\r\n",
      "file: 4300/7500\r\n",
      "file: 4400/7500\r\n",
      "file: 4500/7500\r\n",
      "file: 4600/7500\r\n",
      "file: 4700/7500\r\n",
      "file: 4800/7500\r\n",
      "file: 4900/7500\r\n",
      "file: 5000/7500\r\n",
      "file: 5100/7500\r\n",
      "file: 5200/7500\r\n",
      "file: 5300/7500\r\n",
      "file: 5400/7500\r\n",
      "file: 5500/7500\r\n",
      "file: 5600/7500\r\n",
      "file: 5700/7500\r\n",
      "file: 5800/7500\r\n",
      "file: 5900/7500\r\n",
      "file: 6000/7500\r\n",
      "file: 6100/7500\r\n",
      "file: 6200/7500\r\n",
      "file: 6300/7500\r\n",
      "file: 6400/7500\r\n",
      "file: 6500/7500\r\n",
      "file: 6600/7500\r\n",
      "file: 6700/7500\r\n",
      "file: 6800/7500\r\n",
      "file: 6900/7500\r\n",
      "file: 7000/7500\r\n",
      "file: 7100/7500\r\n",
      "file: 7200/7500\r\n",
      "file: 7300/7500\r\n",
      "file: 7400/7500\r\n",
      "file: 7500/7500\r\n",
      "end\r\n"
     ]
    }
   ],
   "source": [
    "!python SetPreparation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eed0269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T04:31:18.498387Z",
     "iopub.status.busy": "2022-05-17T04:31:18.498155Z",
     "iopub.status.idle": "2022-05-17T04:31:18.504427Z",
     "shell.execute_reply": "2022-05-17T04:31:18.503220Z"
    },
    "papermill": {
     "duration": 0.060089,
     "end_time": "2022-05-17T04:31:18.506640",
     "exception": false,
     "start_time": "2022-05-17T04:31:18.446551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/FaceLandmark_PFLD_UltraLight\n"
     ]
    }
   ],
   "source": [
    "cd /kaggle/working/FaceLandmark_PFLD_UltraLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db67408b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T04:31:18.604118Z",
     "iopub.status.busy": "2022-05-17T04:31:18.603909Z",
     "iopub.status.idle": "2022-05-17T04:31:18.609585Z",
     "shell.execute_reply": "2022-05-17T04:31:18.608887Z"
    },
    "papermill": {
     "duration": 0.055936,
     "end_time": "2022-05-17T04:31:18.611191",
     "exception": false,
     "start_time": "2022-05-17T04:31:18.555255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"config.py\",'w')\n",
    "text='''\n",
    "from torchvision import transforms as trans\n",
    "from easydict import EasyDict as edict\n",
    "from pfld.utils import get_time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    cfg = edict()\n",
    "    cfg.SEED = 2020\n",
    "    cfg.DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    cfg.TRANSFORM = trans.Compose([trans.ToTensor(),\n",
    "                                   trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "    cfg.MODEL_TYPE = 'PFLD_Ultralight'  # [PFLD, PFLD_Ultralight, PFLD_Ultralight_Slim]\n",
    "    cfg.INPUT_SIZE = [96, 96]\n",
    "    cfg.WIDTH_FACTOR = 0.25\n",
    "    cfg.LANDMARK_NUMBER = 98\n",
    "\n",
    "    cfg.TRAIN_BATCH_SIZE = 256\n",
    "    cfg.VAL_BATCH_SIZE = 8\n",
    "\n",
    "    cfg.TRAIN_DATA_PATH = './data/train_data/list.txt'\n",
    "    cfg.VAL_DATA_PATH = './data/test_data/list.txt'\n",
    "\n",
    "    cfg.EPOCHES = 200\n",
    "    cfg.LR = 1e-4\n",
    "    cfg.WEIGHT_DECAY = 1e-6\n",
    "    cfg.NUM_WORKERS = 8\n",
    "    cfg.MILESTONES = [90, 140, 170]\n",
    "\n",
    "    cfg.RESUME = False\n",
    "    if cfg.RESUME:\n",
    "        cfg.RESUME_MODEL_PATH = ''\n",
    "\n",
    "    create_time = get_time()\n",
    "    cfg.MODEL_PATH = './checkpoint/models/{}_{}_{}_{}/'.format(cfg.MODEL_TYPE, cfg.WIDTH_FACTOR, cfg.INPUT_SIZE[0], create_time)\n",
    "    cfg.LOG_PATH = './checkpoint/log/{}_{}_{}_{}/'.format(cfg.MODEL_TYPE, cfg.WIDTH_FACTOR, cfg.INPUT_SIZE[0], create_time)\n",
    "    cfg.LOGGER_PATH = os.path.join(cfg.MODEL_PATH, \"train.log\")\n",
    "    if not os.path.exists(cfg.MODEL_PATH):\n",
    "        os.makedirs(cfg.MODEL_PATH)\n",
    "\n",
    "    return cfg\n",
    "'''\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2421513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T04:31:18.708511Z",
     "iopub.status.busy": "2022-05-17T04:31:18.708314Z",
     "iopub.status.idle": "2022-05-17T04:31:18.717397Z",
     "shell.execute_reply": "2022-05-17T04:31:18.716750Z"
    },
    "papermill": {
     "duration": 0.060015,
     "end_time": "2022-05-17T04:31:18.718952",
     "exception": false,
     "start_time": "2022-05-17T04:31:18.658937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f=open(\"train.py\",'w')\n",
    "text = '''\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from config import get_config\n",
    "from dataset.datasets import WLFWDatasets\n",
    "from pfld.utils import init_weights, save_checkpoint, set_logger, write_cfg\n",
    "from pfld.loss import LandmarkLoss\n",
    "from test import compute_nme\n",
    "\n",
    "from models.PFLD import PFLD\n",
    "from models.PFLD_Ultralight import PFLD_Ultralight\n",
    "from models.PFLD_Ultralight_Slim import PFLD_Ultralight_Slim\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, loss_fn, optimizer, cfg):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_dataloader)) as t:\n",
    "        for img, landmark_gt in train_dataloader:\n",
    "            img = img.to(cfg.DEVICE)\n",
    "            landmark_gt = landmark_gt.to(cfg.DEVICE)\n",
    "            landmark_pred = model(img)\n",
    "            loss = loss_fn(landmark_gt, landmark_pred)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            t.update()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def validate(model, val_dataloader, loss_fn, cfg):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    nme_list = []\n",
    "    with torch.no_grad():\n",
    "        for img, landmark_gt in val_dataloader:\n",
    "            img = img.to(cfg.DEVICE)\n",
    "            landmark_gt = landmark_gt.to(cfg.DEVICE)\n",
    "            landmark_pred = model(img)\n",
    "            loss = loss_fn(landmark_gt, landmark_pred)\n",
    "            losses.append(loss.cpu().numpy())\n",
    "\n",
    "            landmark_pred = landmark_pred.reshape(landmark_pred.shape[0], -1, 2).cpu().numpy()\n",
    "            landmark_gt = landmark_gt.reshape(landmark_gt.shape[0], -1, 2).cpu().numpy()\n",
    "            nme_temp = compute_nme(landmark_pred, landmark_gt)\n",
    "            for item in nme_temp:\n",
    "                nme_list.append(item)\n",
    "\n",
    "    return np.mean(losses), np.mean(nme_list)\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = get_config()\n",
    "\n",
    "    SEED = cfg.SEED\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    set_logger(cfg.LOGGER_PATH)\n",
    "    write_cfg(logging, cfg)\n",
    "\n",
    "    main_worker(cfg)\n",
    "\n",
    "\n",
    "def main_worker(cfg):\n",
    "    # ======= LOADING DATA ======= #\n",
    "    logging.warning('=======>>>>>>> Loading Training and Validation Data')\n",
    "    TRAIN_DATA_PATH = cfg.TRAIN_DATA_PATH\n",
    "    VAL_DATA_PATH = cfg.VAL_DATA_PATH\n",
    "    TRANSFORM = cfg.TRANSFORM\n",
    "\n",
    "    train_dataset = WLFWDatasets(TRAIN_DATA_PATH, TRANSFORM)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=cfg.TRAIN_BATCH_SIZE, shuffle=True, num_workers=cfg.NUM_WORKERS, drop_last=False)\n",
    "\n",
    "    val_dataset = WLFWDatasets(VAL_DATA_PATH, TRANSFORM)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=cfg.VAL_BATCH_SIZE, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "\n",
    "    # ======= MODEL ======= #\n",
    "    MODEL_DICT = {'PFLD': PFLD,\n",
    "                  'PFLD_Ghost': PFLD_Ghost,\n",
    "                  'PFLD_Ghost_Slim': PFLD_Ghost_Slim,\n",
    "                  }\n",
    "    MODEL_TYPE = cfg.MODEL_TYPE\n",
    "    WIDTH_FACTOR = cfg.WIDTH_FACTOR\n",
    "    INPUT_SIZE = cfg.INPUT_SIZE\n",
    "    LANDMARK_NUMBER = cfg.LANDMARK_NUMBER\n",
    "    model = MODEL_DICT[MODEL_TYPE](WIDTH_FACTOR, INPUT_SIZE[0], LANDMARK_NUMBER).to(cfg.DEVICE)\n",
    "    # model.apply(init_weights)\n",
    "    if cfg.RESUME:\n",
    "        if os.path.isfile(cfg.RESUME_MODEL_PATH):\n",
    "            model.load_state_dict(torch.load(cfg.RESUME_MODEL_PATH))\n",
    "        else:\n",
    "            logging.warning(\"MODEL: No Checkpoint Found at '{}\".format(cfg.RESUME_MODEL_PATH))\n",
    "    logging.warning('=======>>>>>>> {} Model Generated'.format(MODEL_TYPE))\n",
    "\n",
    "    # ======= LOSS ======= #\n",
    "    loss_fn = LandmarkLoss(LANDMARK_NUMBER)\n",
    "    logging.warning('=======>>>>>>> Loss Function Generated')\n",
    "\n",
    "    # ======= OPTIMIZER ======= #\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{'params': model.parameters()}],\n",
    "        lr=cfg.LR,\n",
    "        weight_decay=cfg.WEIGHT_DECAY)\n",
    "    logging.warning('=======>>>>>>> Optimizer Generated')\n",
    "\n",
    "    # ======= SCHEDULER ======= #\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.MILESTONES, gamma=0.1)\n",
    "    logging.warning('=======>>>>>>> Scheduler Generated' + '\\n')\n",
    "\n",
    "    # ======= TENSORBOARDX WRITER ======= #\n",
    "    writer = SummaryWriter(cfg.LOG_PATH)\n",
    "\n",
    "    dummy_input = torch.rand(1, 3, INPUT_SIZE[0], INPUT_SIZE[1]).to(cfg.DEVICE)\n",
    "    writer.add_graph(model, (dummy_input,))\n",
    "\n",
    "    best_nme = float('inf')\n",
    "    for epoch in range(1, cfg.EPOCHES + 1):\n",
    "        logging.warning('Epoch {} Start'.format(epoch))\n",
    "        train_loss = train(model, train_dataloader, loss_fn, optimizer, cfg)\n",
    "        val_loss, val_nme = validate(model, val_dataloader, loss_fn, cfg)\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_nme < best_nme:\n",
    "            best_nme = val_nme\n",
    "            save_checkpoint(cfg, model, extra='best')\n",
    "            logging.info('Save best model')\n",
    "        save_checkpoint(cfg, model, epoch)\n",
    "\n",
    "        writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        writer.add_scalar('Train_Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Val_Loss', val_loss, epoch)\n",
    "        writer.add_scalar('Val_NME', val_nme, epoch)\n",
    "\n",
    "        logging.info('Train_Loss: {}'.format(train_loss))\n",
    "        logging.info('Val_Loss: {}'.format(val_loss))\n",
    "        logging.info('Val_NME: {}'.format(val_nme) + '\\n')\n",
    "\n",
    "    save_checkpoint(cfg, model, extra='final')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15c174fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T04:31:18.818828Z",
     "iopub.status.busy": "2022-05-17T04:31:18.818639Z",
     "iopub.status.idle": "2022-05-17T04:31:18.827246Z",
     "shell.execute_reply": "2022-05-17T04:31:18.826498Z"
    },
    "papermill": {
     "duration": 0.061255,
     "end_time": "2022-05-17T04:31:18.829028",
     "exception": false,
     "start_time": "2022-05-17T04:31:18.767773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f=open(\"test.py\",'w')\n",
    "text = '''\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.integrate import simps\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from dataset.datasets import WLFWDatasets\n",
    "\n",
    "from models.PFLD import PFLD\n",
    "from models.PFLD_Ultralight import PFLD_Ultralight\n",
    "from models.PFLD_Ultralight_Slim import PFLD_Ultralight_Slim\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.determinstic = True\n",
    "cudnn.enabled = True\n",
    "\n",
    "\n",
    "def compute_nme(preds, target):\n",
    "    \"\"\" preds/target:: numpy array, shape is (N, L, 2)\n",
    "        N: batchsize L: num of landmark\n",
    "    \"\"\"\n",
    "    N = preds.shape[0]\n",
    "    L = preds.shape[1]\n",
    "    rmse = np.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        pts_pred, pts_gt = preds[i,], target[i,]\n",
    "        if L == 19:  # aflw\n",
    "            interocular = 34  # meta['box_size'][i]\n",
    "        elif L == 29:  # cofw\n",
    "            interocular = np.linalg.norm(pts_gt[8,] - pts_gt[9,])\n",
    "        elif L == 68:  # 300w\n",
    "            # interocular\n",
    "            interocular = np.linalg.norm(pts_gt[36,] - pts_gt[45,])\n",
    "        elif L == 98:\n",
    "            interocular = np.linalg.norm(pts_gt[60,] - pts_gt[72,])\n",
    "        else:\n",
    "            print(L)\n",
    "            raise ValueError('Number of landmarks is wrong')\n",
    "        rmse[i] = np.sum(np.linalg.norm(pts_pred - pts_gt, axis=1)) / (interocular * L)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def compute_auc(errors, failureThreshold, step=0.0001, showCurve=True):\n",
    "    nErrors = len(errors)\n",
    "    xAxis = list(np.arange(0., failureThreshold + step, step))\n",
    "    ced = [float(np.count_nonzero([errors <= x])) / nErrors for x in xAxis]\n",
    "\n",
    "    AUC = simps(ced, x=xAxis) / failureThreshold\n",
    "    failureRate = 1. - ced[-1]\n",
    "\n",
    "    if showCurve:\n",
    "        plt.plot(xAxis, ced)\n",
    "        plt.show()\n",
    "\n",
    "    return AUC, failureRate\n",
    "\n",
    "\n",
    "def validate(model, wlfw_val_dataloader, args):\n",
    "    model.eval()\n",
    "\n",
    "    nme_list = []\n",
    "    cost_time = []\n",
    "    with torch.no_grad():\n",
    "        idx = 0\n",
    "        for img, landmark_gt in wlfw_val_dataloader:\n",
    "            img = img.to(args.device)\n",
    "            landmark_gt = landmark_gt.to(args.device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            landmarks = model(img)\n",
    "            cost_time.append(time.time() - start_time)\n",
    "\n",
    "            landmarks = landmarks.cpu().numpy()\n",
    "            landmarks = landmarks.reshape(landmarks.shape[0], -1, 2)\n",
    "            landmark_gt = landmark_gt.reshape(landmark_gt.shape[0], -1, 2).cpu().numpy()\n",
    "\n",
    "            if args.show_image:\n",
    "                show_img = np.array(np.transpose((img[0] * 0.5 + 0.5).cpu().numpy(), (1, 2, 0)))\n",
    "                show_img = (show_img * 255).astype(np.uint8)\n",
    "                np.clip(show_img, 0, 255)\n",
    "\n",
    "                pre_landmark = landmarks[0, :, :2] * [args.input_size, args.input_size]\n",
    "\n",
    "                cv2.imwrite(\"xxx.jpg\", show_img)\n",
    "                img_clone = cv2.imread(\"xxx.jpg\")\n",
    "\n",
    "                for ptidx, (x, y) in enumerate(pre_landmark):\n",
    "                    cv2.circle(img_clone, (int(x), int(y)), 1, (0, 0, 255), -1)\n",
    "                cv2.imwrite(\"xx_{}.jpg\".format(idx), img_clone)\n",
    "\n",
    "            nme_temp = compute_nme(landmarks, landmark_gt[:, :, :2])\n",
    "            for item in nme_temp:\n",
    "                nme_list.append(item)\n",
    "            idx += 1\n",
    "        # nme\n",
    "        print('nme: {:.4f}'.format(np.mean(nme_list)))\n",
    "        # auc and failure rate\n",
    "        failureThreshold = 0.1\n",
    "        auc, failure_rate = compute_auc(nme_list, failureThreshold)\n",
    "        print('auc @ {:.1f} failureThreshold: {:.4f}'.format(failureThreshold, auc))\n",
    "        print('failure_rate: {:}'.format(failure_rate))\n",
    "        # inference time\n",
    "        print(\"inference_cost_time: {0:4f}\".format(np.mean(cost_time)))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    MODEL_DICT = {'PFLD': PFLD,\n",
    "                  'PFLD_Ghost': PFLD_Ghost,\n",
    "                  'PFLD_Ghost_Slim': PFLD_Ghost_Slim,\n",
    "                  }\n",
    "    MODEL_TYPE = args.model_type\n",
    "    WIDTH_FACTOR = args.width_factor\n",
    "    INPUT_SIZE = args.input_size\n",
    "    LANDMARK_NUMBER = args.landmark_number\n",
    "    model = MODEL_DICT[MODEL_TYPE](WIDTH_FACTOR, INPUT_SIZE, LANDMARK_NUMBER).to(args.device)\n",
    "\n",
    "    checkpoint = torch.load(args.model_path, map_location=args.device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    wlfw_val_dataset = WLFWDatasets(args.test_dataset, transform)\n",
    "    wlfw_val_dataloader = DataLoader(wlfw_val_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "    validate(model, wlfw_val_dataloader, args)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Testing')\n",
    "    parser.add_argument('--model_type', default='PFLD_Ghost', type=str)\n",
    "    parser.add_argument('--input_size', default=112, type=int)\n",
    "    parser.add_argument('--width_factor', default=1, type=float)\n",
    "    parser.add_argument('--landmark_number', default=98, type=int)\n",
    "    parser.add_argument('--device', default='cuda', type=str)\n",
    "    parser.add_argument('--model_path', default=\"./checkpoint/models/PFLD_Ghost_1_112_2020-08-29-08-49/pfld_ghost_best.pth\", type=str)\n",
    "    parser.add_argument('--test_dataset', default='../../data/face_alignment/test_data/list.txt', type=str)\n",
    "    parser.add_argument('--show_image', default=True, type=bool)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n",
    "'''\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "110a06d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T04:31:18.925606Z",
     "iopub.status.busy": "2022-05-17T04:31:18.925405Z",
     "iopub.status.idle": "2022-05-17T04:31:19.666037Z",
     "shell.execute_reply": "2022-05-17T04:31:19.665039Z"
    },
    "papermill": {
     "duration": 0.790985,
     "end_time": "2022-05-17T04:31:19.668066",
     "exception": false,
     "start_time": "2022-05-17T04:31:18.877081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"train.py\", line 131\r\n",
      "    logging.warning('=======>>>>>>> Scheduler Generated' + '\r\n",
      "                                                           ^\r\n",
      "SyntaxError: EOL while scanning string literal\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e969eca",
   "metadata": {
    "papermill": {
     "duration": 0.054936,
     "end_time": "2022-05-17T04:31:19.771699",
     "exception": false,
     "start_time": "2022-05-17T04:31:19.716763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2167.111754,
   "end_time": "2022-05-17T04:31:20.349155",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-17T03:55:13.237401",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
